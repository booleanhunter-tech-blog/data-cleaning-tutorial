[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "",
    "text": "“Data is the new oil!”, they say, painting it as the big-shot driver of the economy. But when you hear that, you’re probably picturing slick, refined gasoline, right?\nWrong picture.\nIn the real world, raw data is like crude oil – unrefined, messy, and full of crud. If not properly cleaned, guess what? You risk clogging the very engine of decision-making you set out to fuel.\nWhen it comes to data analysis, the adage “garbage in, garbage out” couldn’t be more relevant!\nConsider this: bad data costs the US a staggering 3 trillion USD annually! Without thorough cleaning, data becomes not just useless but potentially detrimental, leading to misguided insights and costly business blunders. It’s no surprise, then, that data cleaning is often the first, non-negotiable step in any data science or machine learning endeavor.\nSo in this tutorial, you’ll learn the basics of scrubbing your data clean of inaccuracies and inconsistencies. You’ll explore techniques that cut through the crap, using tools in Python that do the heavy lifting for you. Let’s begin!"
  },
  {
    "objectID": "index.html#tutorial-prerequisites",
    "href": "index.html#tutorial-prerequisites",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Tutorial prerequisites",
    "text": "Tutorial prerequisites\nI’m assuming that you already have some knowledge of programming. Some programming knowledge of Python is necessary, so if you know it, you’ll find this tutorial relatively simple. If you don’t, I highly recommend that you check out this free course on Introduction to Python.\nWhy Python? Because it is fast emerging as the preferred choice of language for data science. It is fairly easy to pick up and learn, and the Python ecosystem has a lot of tools and libraries for virtually building anything — ranging from web servers, packages for statistics, data cleaning, and machine learning. Python has also one of the most active communities on the internet, like stack overflow."
  },
  {
    "objectID": "index.html#launch-the-code",
    "href": "index.html#launch-the-code",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Launch the code",
    "text": "Launch the code\nClone this repository, and follow the instructions to install and run the code. But if that’s too cumbersome -\n\nJust click the button below, which will automatically open an interactive executable environment, right within your browser! Much better, right?\n\n\n\nBinder"
  },
  {
    "objectID": "index.html#the-scenario",
    "href": "index.html#the-scenario",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "The scenario",
    "text": "The scenario\nImagine that the data science and machine learning team at FutureBank want to understand the key factors influencing a customer’s decision to respond positively to term deposit subscriptions. By analyzing this data, the bank wants to refine its marketing strategies, tailor its offerings, and ultimately enhance customer satisfaction and retention.\nHowever, before the team can dive into algorithms and analytics, they need the data to be squeaky clean. That’s where you come in! Your meticulous data cleaning will set the foundation for the team’s success, empowering the team to craft a marketing masterpiece that resonates with its customers.\n\n\n\n\n\nTimeless quotes often require a modern twist"
  },
  {
    "objectID": "index.html#loading-our-data-toolkit",
    "href": "index.html#loading-our-data-toolkit",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Loading our data toolkit",
    "text": "Loading our data toolkit\nFor this tutorial, you’ll be relying on pandas and numpy, two of the most popular libraries that let you manipulate data in a variety of formats.\n\npandas: pandas is the Swiss Army knife in this toolkit, helping you with filtering, grouping, and aggregation. It’s especially powerful for reading and writing data in various formats, handling missing data, and reshaping or pivoting datasets.\nnumpy: Short for Numerical Python, numpy is fundamental for scientific computing in Python. Its ability to perform complex computations quickly and efficiently makes it invaluable for analyzing large datasets. It will help you sort through any numerical mess.\n\n\n# Import the libraries\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "index.html#exploring-our-data",
    "href": "index.html#exploring-our-data",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Exploring our data",
    "text": "Exploring our data\nThe team has managed to gather some information and provided it to you in the form of a csv file. Here’s what it contains:\n\n\nExpand to view columns\n\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ncustomerid\nA unique identifier for each customer.\n\n\nage\nThe age of the customer.\n\n\nsalary\nAnnual income of the customer.\n\n\nbalance\nCurrent account balance.\n\n\nmarital\nMarital status of the customer.\n\n\njobedu\nA combined field of the customer’s job title and level of education.\n\n\ntargeted\nIndicates whether marketing efforts were targeted at the customer based on data analytics.\n\n\ndefault\nWhether the customer has defaulted on a loan.\n\n\nhousing\nIf the customer has a housing loan.\n\n\nloan\nIf the customer has a personal loan.\n\n\ncontact\nThe method of communication used in the campaign.\n\n\nday\nThe day of the month the customer was last contacted.\n\n\nmonth\nThe month the customer was last contacted.\n\n\nduration\nDuration of the last contact, in seconds.\n\n\ncampaign\nNumber of contacts performed during this campaign for this customer.\n\n\npdays\nNumber of days since the customer was last contacted from a previous campaign.\n\n\nprevious\nNumber of contacts performed before this campaign for this customer.\n\n\npoutcome\nOutcome of the previous marketing campaign.\n\n\nresponse\nIf the customer subscribed to a term deposit.\n\n\n\n\nNow, you’ll use the pandas library to load this file into a DataFrame, and read the first 5 rows:\n\n#read the dataset and check the first five rows\nraw_data = pd.read_csv(\"./dataset/bank_marketing_updated_v1.csv\")\nraw_data.head(n=5)\n\n/tmp/ipykernel_2510602/1749228689.py:2: DtypeWarning: Columns (0,1,2,3,11,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n  raw_data = pd.read_csv(\"./dataset/bank_marketing_updated_v1.csv\")\n\n\n\n\n\n\n\n\n\nbanking marketing\nUnnamed: 1\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\nUnnamed: 10\nUnnamed: 11\nUnnamed: 12\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\nUnnamed: 17\nUnnamed: 18\n\n\n\n\n0\ncustomer id and age.\nNaN\nCustomer salary and balance.\nNaN\nCustomer marital status and job with education...\nNaN\nparticular customer before targeted or not\nNaN\nLoan types: loans or housing loans\nNaN\nContact type\nNaN\nmonth of contact\nduration of call\nNaN\nNaN\nNaN\noutcome of previous contact\nresponse of customer after call happned\n\n\n1\ncustomerid\nage\nsalary\nbalance\nmarital\njobedu\ntargeted\ndefault\nhousing\nloan\ncontact\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\nresponse\n\n\n2\n1\n58\n100000\n2143\nmarried\nmanagement,tertiary\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n261 sec\n1\n-1\n0\nunknown\nno\n\n\n3\n2\n44\n60000\n29\nsingle\ntechnician,secondary\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n151 sec\n1\n-1\n0\nunknown\nno\n\n\n4\n3\n33\n120000\n2\nmarried\nentrepreneur,secondary\nyes\nno\nyes\nyes\nunknown\n5\nmay, 2017\n76 sec\n1\n-1\n0\nunknown\nno\n\n\n\n\n\n\n\n\nraw_data.shape #print the dimensions of the dataset\n\n(45213, 19)"
  },
  {
    "objectID": "index.html#the-data-cleaning-process",
    "href": "index.html#the-data-cleaning-process",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "The data cleaning process",
    "text": "The data cleaning process\nBroadly speaking, the data cleaning process can be divided into 5 steps:\n\n\n\n\ngraph LR\n    A[Fix Rows and Columns] --&gt; B[Fix Missing Values]\n    B --&gt; C[Standardize Values]\n    C --&gt; D[Fix Invalid Values]\n    D --&gt; E[Filter Data]\n\n\n\n\n\n\nFix Rows and Columns: Start by adjusting or removing any unnecessary or incorrectly formatted rows and columns.\nFix Missing Values: Next, identify and handle missing data, either by imputing values or deciding how to deal with these gaps.\nStandardize Values: Ensure that all data follows a consistent format, which could involve normalizing numerical values or standardizing text entries.\nFix Invalid Values: Correct or remove any data that is incorrect or does not fit the expected pattern.\nFilter Data: Finally, filter the data to focus on the relevant subset for your analysis or requirements.\n\nWe’ll begin with the first step:"
  },
  {
    "objectID": "index.html#step-1-fix-rows-and-columns",
    "href": "index.html#step-1-fix-rows-and-columns",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Step 1: Fix rows and columns",
    "text": "Step 1: Fix rows and columns\nFor fixing rows, here’s a checklist:\n\nDelete Summary Rows: Remove rows that summarize data, like ‘Total’ or ‘Subtotal’ rows, which can skew analysis.\nDelete Incorrect Rows: Remove rows that are not part of the actual data, such as repeated header rows or footer information.\nDelete Extra Rows: Get rid of rows that are irrelevant to your analysis, such as blank rows, page numbers, or formatting indicators.\n\nNotice, when we loaded our data, the header column looks a bit weird? It seems like the actual data that we need starts from the 3rd row. The first 2 rows seem to be some extra rows, with some unnamed columns. We’ll need to skip them. Here’s how we can do it:\n\n#skip the first 2 rows and read the data again\nraw_data = pd.read_csv(\"./dataset/bank_marketing_updated_v1.csv\", skiprows= 2)\nraw_data.head(n=5)\n\n\n\n\n\n\n\n\ncustomerid\nage\nsalary\nbalance\nmarital\njobedu\ntargeted\ndefault\nhousing\nloan\ncontact\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\nresponse\n\n\n\n\n0\n1\n58.0\n100000\n2143\nmarried\nmanagement,tertiary\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n261 sec\n1\n-1\n0\nunknown\nno\n\n\n1\n2\n44.0\n60000\n29\nsingle\ntechnician,secondary\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n151 sec\n1\n-1\n0\nunknown\nno\n\n\n2\n3\n33.0\n120000\n2\nmarried\nentrepreneur,secondary\nyes\nno\nyes\nyes\nunknown\n5\nmay, 2017\n76 sec\n1\n-1\n0\nunknown\nno\n\n\n3\n4\n47.0\n20000\n1506\nmarried\nblue-collar,unknown\nno\nno\nyes\nno\nunknown\n5\nmay, 2017\n92 sec\n1\n-1\n0\nunknown\nno\n\n\n4\n5\n33.0\n0\n1\nsingle\nunknown,unknown\nno\nno\nno\nno\nunknown\n5\nmay, 2017\n198 sec\n1\n-1\n0\nunknown\nno\n\n\n\n\n\n\n\n\n# We can also print the column names this way:\nraw_data.columns\n\nIndex(['customerid', 'age', 'salary', 'balance', 'marital', 'jobedu',\n       'targeted', 'default', 'housing', 'loan', 'contact', 'day', 'month',\n       'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'response'],\n      dtype='object')\n\n\nNext, notice that the jobedu column is a mashup of job and education details. For a proper analysis, it would be beneficial to separate this into two distinct columns: one for job and another for education. Analyzing job and education individually could play a crucial role in identifying customer segments more likely to respond positively to term deposits.\n\n# Isolate job details into a newly created 'job' column from \"jobedu\" column\nraw_data['job'] = raw_data.jobedu.apply(lambda x: x.split(\",\")[0])\nraw_data.head()\n\n\n\n\n\n\n\n\ncustomerid\nage\nsalary\nbalance\nmarital\njobedu\ntargeted\ndefault\nhousing\nloan\ncontact\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\nresponse\njob\n\n\n\n\n0\n1\n58.0\n100000\n2143\nmarried\nmanagement,tertiary\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n261 sec\n1\n-1\n0\nunknown\nno\nmanagement\n\n\n1\n2\n44.0\n60000\n29\nsingle\ntechnician,secondary\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n151 sec\n1\n-1\n0\nunknown\nno\ntechnician\n\n\n2\n3\n33.0\n120000\n2\nmarried\nentrepreneur,secondary\nyes\nno\nyes\nyes\nunknown\n5\nmay, 2017\n76 sec\n1\n-1\n0\nunknown\nno\nentrepreneur\n\n\n3\n4\n47.0\n20000\n1506\nmarried\nblue-collar,unknown\nno\nno\nyes\nno\nunknown\n5\nmay, 2017\n92 sec\n1\n-1\n0\nunknown\nno\nblue-collar\n\n\n4\n5\n33.0\n0\n1\nsingle\nunknown,unknown\nno\nno\nno\nno\nunknown\n5\nmay, 2017\n198 sec\n1\n-1\n0\nunknown\nno\nunknown\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the code\n\nThe apply() method is used to apply a function along an axis of the DataFrame. In this case, the function is a lambda function, which is an anonymous function defined in-line.\n\nlambda x: x.split(\",\")[0]: This lambda function takes an input x (which represents each element in the jobedu column) and splits the string into a list at each comma.\n[0] accesses the first element of the resulting list (which contains the job info).\n\n\n\n# Isolate education in newly created 'education' column from \"jobedu\" column.\n\nraw_data['education'] = raw_data.jobedu.apply(lambda x: x.split(\",\")[1])\nraw_data.head()\n\n\n\n\n\n\n\n\ncustomerid\nage\nsalary\nbalance\nmarital\njobedu\ntargeted\ndefault\nhousing\nloan\n...\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\nresponse\njob\neducation\n\n\n\n\n0\n1\n58.0\n100000\n2143\nmarried\nmanagement,tertiary\nyes\nno\nyes\nno\n...\n5\nmay, 2017\n261 sec\n1\n-1\n0\nunknown\nno\nmanagement\ntertiary\n\n\n1\n2\n44.0\n60000\n29\nsingle\ntechnician,secondary\nyes\nno\nyes\nno\n...\n5\nmay, 2017\n151 sec\n1\n-1\n0\nunknown\nno\ntechnician\nsecondary\n\n\n2\n3\n33.0\n120000\n2\nmarried\nentrepreneur,secondary\nyes\nno\nyes\nyes\n...\n5\nmay, 2017\n76 sec\n1\n-1\n0\nunknown\nno\nentrepreneur\nsecondary\n\n\n3\n4\n47.0\n20000\n1506\nmarried\nblue-collar,unknown\nno\nno\nyes\nno\n...\n5\nmay, 2017\n92 sec\n1\n-1\n0\nunknown\nno\nblue-collar\nunknown\n\n\n4\n5\n33.0\n0\n1\nsingle\nunknown,unknown\nno\nno\nno\nno\n...\n5\nmay, 2017\n198 sec\n1\n-1\n0\nunknown\nno\nunknown\nunknown\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n#drop the \"jobedu\" column from the dataframe.\nraw_data.drop('jobedu',axis= 1, inplace= True)\nraw_data.head()\n\n\n\n\n\n\n\n\ncustomerid\nage\nsalary\nbalance\nmarital\ntargeted\ndefault\nhousing\nloan\ncontact\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\nresponse\njob\neducation\n\n\n\n\n0\n1\n58.0\n100000\n2143\nmarried\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n261 sec\n1\n-1\n0\nunknown\nno\nmanagement\ntertiary\n\n\n1\n2\n44.0\n60000\n29\nsingle\nyes\nno\nyes\nno\nunknown\n5\nmay, 2017\n151 sec\n1\n-1\n0\nunknown\nno\ntechnician\nsecondary\n\n\n2\n3\n33.0\n120000\n2\nmarried\nyes\nno\nyes\nyes\nunknown\n5\nmay, 2017\n76 sec\n1\n-1\n0\nunknown\nno\nentrepreneur\nsecondary\n\n\n3\n4\n47.0\n20000\n1506\nmarried\nno\nno\nyes\nno\nunknown\n5\nmay, 2017\n92 sec\n1\n-1\n0\nunknown\nno\nblue-collar\nunknown\n\n\n4\n5\n33.0\n0\n1\nsingle\nno\nno\nno\nno\nunknown\n5\nmay, 2017\n198 sec\n1\n-1\n0\nunknown\nno\nunknown\nunknown\n\n\n\n\n\n\n\n\n\ninplace=True is useful for making changes to the original DataFrame without having to create a new one"
  },
  {
    "objectID": "index.html#step-2-fix-missing-values",
    "href": "index.html#step-2-fix-missing-values",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Step 2: Fix missing values",
    "text": "Step 2: Fix missing values\nMissing values are like puzzle pieces that got lost – they can leave you with an incomplete picture. But don’t worry, there are four strategic ways to bring everything back into focus:\n\nMark the Absentees: Scour through your dataset and flag the elusive ones. For instance, one might have marked a value as NA or some placeholder values like XXX, or 999.\nTrim the excess: When a few data points have wandered off leaving barely a trace, it’s safe to let them go. Such rows can be deleted if the number of missing values is insignificant, as this would not impact the overall analysis results. But Columns can be treated a bit differently - you can get rid of one if the missing values are significant in number.\nFill in the blanks with business insight: If you have a good understanding of your domain, you can make informed guesses that make sense.\nFill missing values using statistics: You can also make informed guesses by observing patterns in your data, and filling in the missing values with a close approximation. This technique is also called imputation.\n\nLet’s see which columns in the dataset contain null values:\n\nraw_data.isnull().sum()\n\ncustomerid     0\nage           20\nsalary         0\nbalance        0\nmarital        0\ntargeted       0\ndefault        0\nhousing        0\nloan           0\ncontact        0\nday            0\nmonth         50\nduration       0\ncampaign       0\npdays          0\nprevious       0\npoutcome       0\nresponse      30\njob            0\neducation      0\ndtype: int64\n\n\n\n\n\nisnull returns a new DataFrame of booleans, where a cell is True if the corresponding cell in raw_data is a missing value (NaN, None, or NaT), and False otherwise.\nIn pandas, when you call .sum() on a DataFrame of booleans, it treats True as 1 and False as 0.\n\nSo it appears that age, month, and response all have missing values. Let’s handle the age column first. How much percent of the age column is actually missing?\nFor this, we’ll need to compute\n\\[\n\\frac{\\text{missing values in the age column}}{\\text{total number of rows}} \\times 100\n\\]\n\n#calculate the percentage of missing values in age column.\nmissing_ages_count = raw_data.age.isnull().sum()\nmissing_ages_count * 100/raw_data.shape[0]\n\n0.04423702196368141\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlternatively, you can do this too:\nraw_data.age.isnull().mean() * 100\n\n\nAs we can see, they are quite small in number. The easiest way to deal with them is to just delete them.\n\n#drop the records with age missing in raw_data\n\nraw_data = raw_data[-raw_data.age.isnull()].copy()\n\nprint('Number of null values in age after dropping: ', raw_data.age.isnull().sum())\nprint('Total number of rows:', raw_data.shape[0])\n\nNumber of null values in age after dropping:  0\nTotal number of rows: 45191\n\n\n\n\n\n\nWhat does this code do?\n\n\nraw_data.age.isnull(): This part of the code returns a boolean series where each value is True if the corresponding value in the age column of raw_data is missing, and False otherwise.\nThe - sign in front is used for negating the boolean series. In other words, it converts True to False and vice versa. Thus, this operation effectively selects all rows where the age column does not have missing values.\nAs a result, raw_data[-raw_data.age.isnull()] creates a new DataFrame that includes only those rows of raw_data where the age value is not null.\n\nFinally, we use copy() method to create a new copy of this dataframe, and store it back in the original one.\n\nNext, we’ll look at the month column. How many records are missing?\n\n#count the missing values in month column.\nraw_data.month.isnull().mean() * 100\n\n0.11064149941360005\n\n\nDepending on what we want, we could try different approaches.\n\nWe could once again, simply drop the rows with missing valaues.\nOr, we could fill them (imputation).\n\nLet’s try the imputation strategy this time! But how do we know what value to impute?\nLet’s analyze the values in the month columnn and how frequently they appear in the dataset. Here are a few techniques:\n\nWe could replace the missing values with the value that occurs most frequently (mode).\nFor columns containing quantitative values, we might also have the option to replace the missing values by the average.\n\n\nraw_data.month.value_counts(normalize = True)\n\nmay, 2017    0.304380\njul, 2017    0.152522\naug, 2017    0.138123\njun, 2017    0.118141\nnov, 2017    0.087880\napr, 2017    0.064908\nfeb, 2017    0.058616\njan, 2017    0.031058\noct, 2017    0.016327\nsep, 2017    0.012760\nmar, 2017    0.010545\ndec, 2017    0.004741\nName: month, dtype: float64\n\n\n\n# Find the value that occurs most frequently (mode)\n\nmonth_mode=raw_data.month.mode()[0]\nprint(month_mode)\n\nmay, 2017\n\n\nIn this case, the value may, 2017 appears most frequently (about 30%) of the time. This is a good value to impute.\nLet’s replace all missing values in the month column with this value.\n\n# fill the missing values with mode value of month in raw_data.\nraw_data.month.fillna(month_mode, inplace= True)\n\nprint('Number of null values in month after dropping: ', raw_data.month.isnull().sum())\nprint(raw_data.month.value_counts(normalize= True))\n\nNumber of null values in month after dropping:  0\nmay, 2017    0.305149\njul, 2017    0.152353\naug, 2017    0.137970\njun, 2017    0.118010\nnov, 2017    0.087783\napr, 2017    0.064836\nfeb, 2017    0.058551\njan, 2017    0.031024\noct, 2017    0.016309\nsep, 2017    0.012746\nmar, 2017    0.010533\ndec, 2017    0.004735\nName: month, dtype: float64\n\n\n\n\n\nfillna is a method in pandas used to fill missing values (denoted as NaN) in a DataFrame or Series.\nThe inplace=True argument specifies that this operation should modify the DataFrame directly, instead of creating a new one with the missing values filled. This means that after this line of code is executed, the month column will no longer contain any missing values; they will have been replaced with the value in month_mode.\n\nNow, we have one more column to fix missing values in: response.\n\n#count the missing values in month column.\nraw_data.response.isnull().mean() * 100\n\n0.06638489964816004\n\n\nThis column has less than 7% of null values. We could just drop them off!\n\n#drop the records with missing values for response.\nraw_data = raw_data[~raw_data.response.isnull()]\nprint('Number of null values in response after dropping: ', raw_data.response.isnull().sum())\nprint('Total number of rows:', raw_data.shape[0])\n\nNumber of null values in response after dropping:  0\nTotal number of rows: 45161\n\n\n\nIs that all?\n\nNot quite!\n\n\n\nAssumptions corrupt data\n\n\n\n\n\n\n\n\nNote\n\n\n\nData can often be deceptive. Don’t just assume that missing values will always be present as null or NaN!\n\n\nTake a look at the pdays column, for example -\n\n#describe the pdays column\nraw_data.pdays.describe()\n\ncount    45161.000000\nmean        40.182015\nstd        100.079372\nmin         -1.000000\n25%         -1.000000\n50%         -1.000000\n75%         -1.000000\nmax        871.000000\nName: pdays, dtype: float64\n\n\n\n(raw_data.pdays &lt; 0).mean() * 100\n\n81.74088261995969\n\n\nWoah! Looks like 81% of the values in this column have a value of -1 value in them! Seems like the data gathering team entered -1 to indicate Null values!\nHere’s what we could do:\nwe could simply ignore the missing values in the calculations. For this, we’ll need to replace all the negative values with Nan, so that the number crunching that we might do later on will ignore these missing values.\n\n#describe the pdays column with considering the -1 values.\nraw_data.loc[raw_data.pdays &lt; 0, \"pdays\"] = np.NaN\nraw_data.pdays.describe()\n\ncount    8246.000000\nmean      224.542202\nstd       115.210792\nmin         1.000000\n25%       133.000000\n50%       195.000000\n75%       327.000000\nmax       871.000000\nName: pdays, dtype: float64\n\n\n\n\n\n\nWhat does this code do?\n\n\n.loc[] is a label-based data selecting method used to select rows and columns from a DataFrame.\nraw_data.pdays &lt; 0 is a condition that checks which rows in the pdays column of raw_data have values less than 0. This condition creates a boolean series where each value is True if the corresponding value in pdays is less than 0, and False otherwise.\nBy placing this condition within raw_data.loc[], along with the column name \"pdays\", the code selects all rows with pdays values that are less than 0.\nFinally, np.Nan replaces them with NaN, effectively treating them as missing or undefined values.\n\n\nPerfect! Now you can move on to the next step -"
  },
  {
    "objectID": "index.html#step-3-standardizing-values",
    "href": "index.html#step-3-standardizing-values",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Step 3: Standardizing values",
    "text": "Step 3: Standardizing values\nThe first question to ask is - are there extreme values in our dataset?\n\nDetecting outliers\nFinding out outliers in a dataset is crucial, because very small or very large values can negatively affect our data analysis and skew our findings.\n\nHow outliers skew data analysis\nImagine you’re analyzing the salary data of employees in a tech company. The company has a diverse pool of talent, ranging from fresh graduates to seasoned experts, with salaries mostly ranging between $50,000 and $150,000 annually. Out of 500 employees, 498 have salaries within this range.\nBut out of 500 folks, imagine that there are these two big shots – high-flyers with a compensation that’s in a league of its own, over $2 million each, thanks to a stack of stock options and bonuses. What happens?\nIn this scenario, the two executives are outliers. They’re like the unicorns in a field of horses. Not exactly what you’d call ‘typical’. They are not representative of the typical employee’s earnings and can significantly skew the average salary calculation. So if you were to calculate the average salary including these outliers, you might conclude that the average employee at this company earns around $90,000 a year. But let’s face it - this number is misleading! It’s heavily influenced by the two extreme values.\nThis is why Per capita income isn’t always considered to be a true measure of development.\n\n\nOkay, so how do we handle them?\nThere’s a whole arsenal of tactics at your disposal to tackle these outliers. You could, for example, kick them out of your dataset entirely. Or, you might play it cool and just cap their influence, keeping them in check. Another move? Play the imputation game, where you replace their values with other values that are more ‘typical’, with some savvy guesswork. Or, if you’re feeling strategic, go for binning and categorize them.\nLet’s dive deeper into each of these methods and see what works best:\nWhen it comes to hunting down outliers, Python arms us with some seriously powerful visualization tools. They’re like our digital magnifying glasses, that help us zoom in on those sneaky data points that try to throw a wrench in our analysis.\nFor this task, you’ll be utilizing Plotly. It’s a great tool for interactive data visualization in Python, enabling us to clearly identify and analyze outliers through its interactive plots.\n\n#import plotly\n\nimport plotly.express as px\n\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n\nLet’s plot a histogram - one of the seven basic tools of quality.\n\n# Create a histogram of the age variable using Plotly Express\nfig = px.histogram(raw_data, x='age')\n\n# Show the plot\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nHow does a histogram help?\n\n\n\n\n\nA histogram is a valuable tool in outlier detection for several reasons:\n\nVisual Representation of Distribution: Histograms provide a visual representation of data distribution. They display the frequency of data points within certain ranges (bins), making it easier to see the overall spread of the data.\nIdentifying Skewness and Peaks: Outliers can cause a distribution to be skewed. In a histogram, this might appear as a long tail extending to the right (positive skew) or left (negative skew). A distribution with a significant skew might suggest the presence of outliers.\nSpotting Unusual Patterns: Outliers can manifest as isolated bars on a histogram, separated from the majority of the data. For example, if most data points are clustered within a certain range, but a few bars appear far from this cluster, those bars could represent outliers.\n\n\n\n\nNext, we’ll make use of another plot - the box plot.\n\n# Create a boxplot of the age variable using Plotly Express\nfig = px.box(raw_data, y='age')\n\n# Show the plot\nfig.show()\n\n\n                                                \n\n\n\n\nIn this graph, data points that fall outside of the whiskers are often considered potential outliers. In this plot, they are indicated with dots.\nWhat’s the box plot about?\nThe box plot is another excellent tool for detecting outliers due to its specific way of representing data distribution. The box in a box plot represents the Interquartile range, which is the range between the first and third quartiles (Q3 - Q1). The IQR is crucial in identifying outliers, as it measures the spread of the middle 50% of the data.\n\n\n\n\n\n\nSo what does the graph tell us?\n\n\n\nAlright, let’s get this straight: our data’s showing some ‘outliers’ in the age variable, especially in the 70-90 bracket. But let’s not jump the gun here. People being in their 70s or 80s? Totally normal, happens all the time! This isn’t a teen party; it’s real life, and in real life, people grow old.\nJust because they’re few and far between doesn’t mean they are some statistical fluke. So it would be wise to keep them. They might even be key players in this whole term deposit account game.\n\n\nNext, let’s look at the salary variable:\n\nraw_data.salary.describe()\n\ncount     45161.000000\nmean      57004.849317\nstd       32087.698810\nmin           0.000000\n25%       20000.000000\n50%       60000.000000\n75%       70000.000000\nmax      120000.000000\nName: salary, dtype: float64\n\n\n\n#plot the boxplot of salary variable.\nfig = px.box(raw_data, y='salary')\n\n# Show the plot\nfig.show()\n\n\n                                                \n\n\nNext, examine the balance variable:\n\n#describe the balance variable of inp1.\nraw_data.balance.describe()\n\ncount     45161.000000\nmean       1362.850690\nstd        3045.939589\nmin       -8019.000000\n25%          72.000000\n50%         448.000000\n75%        1428.000000\nmax      102127.000000\nName: balance, dtype: float64\n\n\n\nfig = px.box(raw_data, y='balance')\n\n# Show the plot\nfig.show()\n\n\n                                                \n\n\n\n\n\nYour Turn to Dive In\nWe’ve walked through the nitty-gritty of understanding and handling outliers, and now, it’s your turn to take the reins. Consider this the part where you roll up your sleeves and get your hands dirty with real data. As an exercise, here’s what you can do:\n\nIdentify Potential Outliers: Use your newfound skills to spot potential outliers in other variables of the dataset. Maybe take a closer look at variables like ‘salary’, ‘balance’, or ‘duration’. Are there any unexpected peaks or values that seem off the charts?\nAnalyze Their Impact: Once you’ve identified these outliers, analyze their impact. How do they skew the data? Do they tell a story that’s worth digging into, or are they just statistical noise?\nDecide Your Approach: Based on your analysis, decide how you want to handle these outliers. Will you adjust them, remove them, or leave them as they are? Remember, there’s no one-size-fits-all answer here; it’s about what makes sense in the context of your specific data story.\nReflect and Document: As you work through this exercise, make sure to document your observations and decisions. What did you learn? How did the presence (or absence) of outliers affect your overall analysis?"
  },
  {
    "objectID": "index.html#step-4-fixing-invalid-values",
    "href": "index.html#step-4-fixing-invalid-values",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Step 4: Fixing invalid values",
    "text": "Step 4: Fixing invalid values\nIn the world of data analysis, consistency is king. Why is this important? Because data often comes from various sources, each singing to its own tune – different units, scales, formats, you name it. Our goal here is to orchestrate these diverse elements into a unified language that our analytical tools can understand and interpret accurately.\nNot fixing invalid values in your data can lead to data quality issues, making the data less reliable and harder to work with!\n\nSo here’s what you need to look out for:\n\nFix Units\nEnsure all observations under a single variable are expressed in a consistent unit. Here are some examples:\n\nIf you have weight data in both pounds (lbs) and kilograms (kg), choose one (preferably the one most commonly used in your dataset’s context) and convert all data to that unit.\nIf some data points are recorded on a different scale (e.g., a test score out of 50 vs. 100), convert them to a common scale, like a percentage.\nThis uniformity prevents confusion and errors in analysis that can arise from unit discrepancies.\n\n\n\nFix Precision\nMaintain a consistent level of decimal precision for better readability and uniformity. Round off numerical values to a standard number of decimal places that makes sense for your analysis, e.g., changing 4.5312341 kg to 4.53 kg. It enhances data readability and prevents the overemphasis of minor differences that are not meaningful for the analysis.\n\n\nRemove extra characters\nCleanse string variables of unnecessary characters. Remove any irrelevant characters such as prefixes, suffixes, and extra spaces. For instance, trim spaces from a name string like \" John Doe \".\n\n\nFix inconsistent casing\nUnify the casing style of string variables. Convert all text to a consistent case format (uppercase, lowercase, title case, etc.), based on what’s most appropriate for your data. This is particularly useful for categorical data and text analysis, ensuring that the same categories or keywords are not treated as different due to case differences.\n\n\nStandardize Format\nEnsure consistency in the format of structured text data like dates and names. For example, changing date formats from 23/10/16 to 2016/10/23, or standardizing name formats. This uniformity is crucial for sorting, filtering, and correctly interpreting structured text data.\nLet’s look at an example from our dataset:\n\nraw_data.duration.head(10)\n\n0    261 sec\n1    151 sec\n2     76 sec\n3     92 sec\n4    198 sec\n5    139 sec\n6    217 sec\n7    380 sec\n8     50 sec\n9     55 sec\nName: duration, dtype: object\n\n\nAs you’ll notice, the values in this column are appended with a ‘sec’ suffix. This could pose a challenge when performing numerical calculations.\nA practical solution would be to standardize these values into minutes while simultaneously removing the sec suffix. This way, we’re left with purely numerical values, more conducive to accurate and efficient data analysis.\n\nraw_data.duration = raw_data.duration.apply(lambda x: float(x.split()[0])/60 if x.find(\"sec\") &gt; 0 else float(x.split()[0]))\n\nraw_data.duration.describe()\n\ncount    45161.000000\nmean         4.302774\nstd          4.293129\nmin          0.000000\n25%          1.716667\n50%          3.000000\n75%          5.316667\nmax         81.966667\nName: duration, dtype: float64\n\n\n\n\n\n\nWhat does this code do?\n\n\n.apply(lambda x: ...): The apply method is used to apply a function along an axis of the DataFrame. Here, it’s being applied to each element (x) in the duration column\nThe function splits each string in the duration columnn if the substring 'sec' exists, and takes the first element of the resulting list. This is useful for extracting the numeric part from a string like \"120 sec\"\nfloat(...): Converts the extracted string to a floating-point number."
  },
  {
    "objectID": "index.html#wrapping-up",
    "href": "index.html#wrapping-up",
    "title": "Data Cleaning: How the pros do the dirty work",
    "section": "Wrapping up",
    "text": "Wrapping up\nWe’ve navigated through the crucial steps of data cleaning and standardization, tackling challenges like handling missing values, detecting outliers, and ensuring our data speaks in a consistent and clear language. These steps form the backbone of any robust data analysis process, turning raw data into a reliable foundation for informed decision-making.\nBut remember, this is just the tip of the iceberg. The realm of data analysis is vast and filled with many more techniques, methods, and best practices. What we’ve explored are the highlights, the essential skills that every data enthusiast should master.\nTo further aid your journey in data analysis and to ensure you have quick access to essential information, here’s a handy reference below. This table is a distilled guide to data cleaning, encompassing the key steps and methods we’ve discussed. Keep it as a go-to resource whenever you embark on a data-cleaning exercise. It’s designed to serve as a quick reminder of the various techniques and approaches that are crucial in transforming raw data into a clean, analysis-ready format.\n\n\nThe data-cleaning cheatsheet\n\n\n\n\n\n\n\nData quality issue\nExamples\nHow to resolve\n\n\n\n\nFix rows and columns\n\n\n\n\nIncorrect rows\nHeader rows, footer rows\nDelete\n\n\nSummary rows\nTotal, subtotal rows\nDelete\n\n\nExtra rows\nColumn numbers, indicators, blank rows\nDelete\n\n\nMissing Column Names\nColumn names as blanks, NA, XX etc.\nAdd the column names\n\n\nInconsistent column names\nX1, X2,C4 which give no information about the column\nAdd column names that give some information about the data\n\n\nUnnecessary columns\nUnidentified columns, irrelevant columns, blank columns\nDelete\n\n\nColumns containing Multiple data values\nE.g. address columns containing city, state, country\nSplit columns into components\n\n\nNo Unique Identifier\nE.g. Multiple cities with same name in a column\nCombine columns to create unique identifiers e.g. combine City with the State\n\n\nMisaligned columns\nShifted columns\nAlign these columns\n\n\nMissing Values\n\n\n\n\nDisguised Missing values\nblank strings \"  \", \"NA\", \"XX\", \"999\"\nSet values as missing values\n\n\nSignificant number of Missing values in a row/column\n\nDelete rows, columns\n\n\nPartial missing values\nMissing time zone, century etc\nFill the missing values with the correct value\n\n\nStandardise Numbers\n\n\n\n\nNon-standard units\nConvert lbs to kgs, miles/hr to km/hr\nStandardise the observations so all of them have the same consistent units\n\n\nValues with varying Scales\nA column containing marks in subjects, with some subject marks out of 50 and others out of 100\nMake the scale common. E.g. a percentage scale\n\n\nOver-precision\n4.5312341 kgs, 9.323252 meters\nStandardise precision for better presentation of data. 4.5312341 kgs could be presented as 4.53 kgs\n\n\nRemove outliers\nAbnormally High and Low values\nCorrect if by mistake else Remove\n\n\nStandardise Text\n\n\n\n\nExtra characters\nCommon prefix/suffix, leading/trailing/multiple spaces\nRemove the extra characters\n\n\nDifferent cases of same words\nUppercase, lowercase, Title Case, Sentence case, etc\nStandardise the case/bring to a common case\n\n\nNon-standard formats\n23/10/16 to 2016/10/20, \"Reacher, Jack\" to \"Jack Reacher\"\nCorrect the format/Standardise format for better readability and analysis\n\n\nFix Invalid Values\n\n\n\n\nEncoding Issues\nCP1252 instead of UTF-8\nEncode unicode properly\n\n\nIncorrect data types\nNumber stored as a string: \"12,300\"\nConvert to Correct data type\n\n\nCorrect values not in list\nNon-existent country, PIN code\nDelete the invalid values, treat as Missing\n\n\nWrong structure\nPhone number with over 10 digits\n\n\n\nCorrect values beyond range\nTemperature less than -273°C (0° K)\n\n\n\nValidate internal rules\nGross sales &gt; Net sales\n\n\n\n\nDate of delivery &gt; Date of ordering\n\n\n\n\nIf Title is \"Mr\" then Gender is \"M\"\n\n\n\nFilter Data\n\n\n\n\nDuplicate data\nIdentical rows, rows where some columns are identical\nDeduplicate Data/ Remove duplicated data\n\n\nExtra/Unnecessary rows\nRows that are not required in the analysis. E.g if observations before or after a particular date only are required for analysis, other rows become unnecessary\nFilter rows to keep only the relevant data.\n\n\nColumns not relevant to analysis\nColumns that are not needed for analysis e.g. Personal Detail columns such as Address, phone column in a dataset for\nFilter columns and pick only the ones relevant to your analysis\n\n\nDispersed data\nParts of data required for analysis stored in different files or part of different datasets\nBring the data together, Group by required keys, aggregate the rest"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Motivations",
    "section": "",
    "text": "This project draws inspiration from Joel Spolsky’s insightful blog post on the importance of a development abstraction layer. While the article was intended mainly for a project manager, I think it applies well even for a developer relations role. Eliminating any obstacles that hinder programmers from doing their job is super important. And often accessing relevant documentation promptly is one of them!\nAs a developer advocate, I understand the need to make it straightforward for newcomers to grasp the concepts and utilize the functionality being described. But along with creating the tutorial, I pondered, why not also explore ways to enhance documentation and improve accessibility? The features incorporated into this website are an experimental exploration of this very idea. Here are some of them:\n\nSearch Feature: As a developer, I frequently find myself recalling that I’ve come across a particular API or method in a tutorial but struggling to pinpoint where. A search feature would be incredibly helpful, eliminating the need for me to remember the exact location of each section.\nTable of Contents: Include a sticky table of contents for easy navigation through the material that can be collapsed by the reader.\nSidenotes: Utilize the large unused margins on the left and right to incorporate sidenotes. These sidenotes can be used to provide quick explanations of code aspects without causing distraction or affecting their flow. Developers may choose to read or skip them based on their preferences (see example on right).\n\n\n\n\n\nCollapsible sidenote\n\n\ndf.age.isnull(): This part of the code returns a boolean series where each value is True if the corresponding value in the age column of df is missing, and False otherwise.\n\n\nThe above 3 features allow you to achieve the skimmable principle, as outlined in Write the Docs.\n\n\n\n\n\n\n4. Callouts\n\n\n\nUse callouts to highlight particularly important or interesting points within the article.\n\n\n\nReport an Issue: Offer a user-friendly “report an issue” button that directs readers to the GitHub repository’s issues tab, if they face any difficulties while navigating the tutorial/docs (See participatory principle from Write the Docs.\nVisual Elements: Enhance engagement by integrating mermaid diagrams and Plotly for visual representation and interactivity. This approach can encourage interaction with the content, transforming readers into participants.\nA dark mode! 😎\n\nWhat are your thoughts on these ideas?\n\n\n\n Back to top"
  }
]